{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9203178,"sourceType":"datasetVersion","datasetId":5564361}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"f1b6e545-c08d-4278-aa0c-e1aa62a547de","cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ArX_QboFGdCS","outputId":"fe81828c-d2d9-43e3-a66a-f2b765f230df","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:12:58.475872Z","iopub.execute_input":"2025-03-11T13:12:58.476151Z","iopub.status.idle":"2025-03-11T13:12:58.486327Z","shell.execute_reply.started":"2025-03-11T13:12:58.476127Z","shell.execute_reply":"2025-03-11T13:12:58.485585Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/New Data1.csv\n/kaggle/input/PAAC new Train.csv\n/kaggle/input/DistancePair Whole new Train.csv\n/kaggle/input/DDE Whole new Train.csv\n/kaggle/input/APAAC Train New Whole.csv\n/kaggle/input/CTDC Whole new Train.csv\n","output_type":"stream"}],"execution_count":1},{"id":"1c8a201d-745b-454b-be53-78ece7a25651","cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport os","metadata":{"id":"71540e79-75bb-4654-87a5-a700fe66571f","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:13:16.780436Z","iopub.execute_input":"2025-03-11T13:13:16.780782Z","iopub.status.idle":"2025-03-11T13:13:17.789301Z","shell.execute_reply.started":"2025-03-11T13:13:16.780753Z","shell.execute_reply":"2025-03-11T13:13:17.788694Z"}},"outputs":[],"execution_count":2},{"id":"4958d172-a8f9-4d5c-ba65-93dd84f5bd19","cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, cohen_kappa_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import matthews_corrcoef\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_classif","metadata":{"id":"61befd95-a284-4d81-95d0-0fb3f91a9a27","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:13:19.525569Z","iopub.execute_input":"2025-03-11T13:13:19.525990Z","iopub.status.idle":"2025-03-11T13:13:19.951991Z","shell.execute_reply.started":"2025-03-11T13:13:19.525964Z","shell.execute_reply":"2025-03-11T13:13:19.951303Z"}},"outputs":[],"execution_count":3},{"id":"888800ef-a2fe-4440-9721-13ddf8269d51","cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import f_classif","metadata":{"id":"839e7f95-1b07-4ae2-9eae-ef29024b675c","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:13:21.621155Z","iopub.execute_input":"2025-03-11T13:13:21.621485Z","iopub.status.idle":"2025-03-11T13:13:21.625293Z","shell.execute_reply.started":"2025-03-11T13:13:21.621455Z","shell.execute_reply":"2025-03-11T13:13:21.624448Z"}},"outputs":[],"execution_count":4},{"id":"d65a21f5-1bd1-43e7-a6e6-7192bd7bed7d","cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV","metadata":{"id":"e3be7379-33ee-4e67-b2c8-e4403785f165","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:13:22.603473Z","iopub.execute_input":"2025-03-11T13:13:22.603841Z","iopub.status.idle":"2025-03-11T13:13:22.632600Z","shell.execute_reply.started":"2025-03-11T13:13:22.603812Z","shell.execute_reply":"2025-03-11T13:13:22.631900Z"}},"outputs":[],"execution_count":5},{"id":"98435bf7-b14b-4ec8-b927-bbdc81612d5c","cell_type":"code","source":"\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import ExtraTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n","metadata":{"id":"be9e1628-7890-453c-b1c5-8a1348d99c70","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:13:22.956692Z","iopub.execute_input":"2025-03-11T13:13:22.956991Z","iopub.status.idle":"2025-03-11T13:13:23.132673Z","shell.execute_reply.started":"2025-03-11T13:13:22.956966Z","shell.execute_reply":"2025-03-11T13:13:23.131884Z"}},"outputs":[],"execution_count":6},{"id":"26fccaac-b73f-44d3-a19d-f4940651dcbb","cell_type":"code","source":"from keras.layers import Dense, Conv1D, MaxPool1D, Flatten, Dropout,LSTM, Input\nfrom keras.layers import Conv1D, LSTM, BatchNormalization, MaxPooling1D, Dense, Reshape\nfrom keras.models import Sequential\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\n","metadata":{"id":"b65c8645-f371-4093-830a-0c29265b0fd9","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:13:24.118019Z","iopub.execute_input":"2025-03-11T13:13:24.118306Z","iopub.status.idle":"2025-03-11T13:13:35.712566Z","shell.execute_reply.started":"2025-03-11T13:13:24.118283Z","shell.execute_reply":"2025-03-11T13:13:35.711687Z"}},"outputs":[],"execution_count":7},{"id":"285e1871-11e4-447d-b1a4-347872e3094e","cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import precision_score, recall_score, f1_score, matthews_corrcoef, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import log_loss","metadata":{"id":"bea2818e-8e94-4acf-8561-37dc47fe0fe3","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:13:35.713731Z","iopub.execute_input":"2025-03-11T13:13:35.714289Z","iopub.status.idle":"2025-03-11T13:13:35.718571Z","shell.execute_reply.started":"2025-03-11T13:13:35.714261Z","shell.execute_reply":"2025-03-11T13:13:35.717761Z"}},"outputs":[],"execution_count":8},{"id":"eb81aaaf-72cb-40d0-8234-6a0213615c1e","cell_type":"code","source":"# from transformers import ViTModel, ViTConfig","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bd388464-cbd1-4061-a6c8-c2da139b6545","cell_type":"code","source":"file_name = r'/kaggle/working/model-5-result.csv'\nvalidation_file = r'/kaggle/working/model-5-validation.csv'\nroc_curve_cv_file = r'/kaggle/working/model-5-roc-curve-cv.csv'\nroc_curve_ts_file = r'/kaggle/working/model-5-roc-curve-ts.csv'","metadata":{"id":"ff86dfe0-e64c-4797-b9ab-7704f880f241","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:13:35.719707Z","iopub.execute_input":"2025-03-11T13:13:35.719977Z","iopub.status.idle":"2025-03-11T13:13:35.749177Z","shell.execute_reply.started":"2025-03-11T13:13:35.719942Z","shell.execute_reply":"2025-03-11T13:13:35.748574Z"}},"outputs":[],"execution_count":9},{"id":"3904d949-d74c-4166-bc37-48bc6d77e9ca","cell_type":"markdown","source":"# Model Evaluation","metadata":{"id":"e6a0b1c0-9d08-4425-9ba1-f62445f9bd52"}},{"id":"f379fb18-1aad-4289-8fcb-6ba695aca26c","cell_type":"code","source":"# for test set\ndef total_matric_cal(ytest,ypred,pred,model_name): \n    pred_clean = np.nan_to_num(pred, nan=0.0)  # Replace NaNs with 0 in pred\n    ytest_cnn_clean = np.nan_to_num(ytest_cnn, nan=0.0)  # Replace NaNs with 0 in ytest_cnn\n    Metrics = []\n    Metrics = pd.DataFrame(Metrics)\n    Metrics['Model'] = 'model'\n    Metrics['Accuracy'] = 'Accuracy'\n    Metrics['mcc'] = 'mcc'\n    Metrics['Kappa'] = 'Kappa'\n    Metrics['precision'] = 'precision'\n    Metrics['recall'] = 'recall'\n    Metrics['f1'] = 'f1'\n    Metrics['sensitivity'] = 'sensitivity'\n    Metrics['specificity'] = 'specificity'\n    Metrics['auc'] = 'auc'\n    Metrics['loss'] = 'loss'\n    Accuracy = accuracy_score(ytest,ypred)\n    mcc = matthews_corrcoef(ytest,ypred)\n    cm1 = confusion_matrix(ytest,ypred)\n    kappa = cohen_kappa_score(ytest,ypred)\n    f1 = f1_score(ytest,ypred, average='macro')\n    precision = precision_score(ytest,ypred, average='macro')\n    recall = recall_score(ytest,ypred, average='macro')\n    sensitivity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n    specificity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n    auc = roc_auc_score(ytest_cnn_clean, pred_clean, multi_class='ovr')\n    # Binary Cross-Entropy Loss\n    loss = log_loss(ytest_cnn_clean, pred_clean)\n    Metrics.loc[len(Metrics.index)] = [model_name,Accuracy, mcc, kappa, precision,recall, f1, sensitivity,specificity,auc,loss]\n    return Metrics","metadata":{"id":"31ca3a23-d3c9-4f6b-b9a1-8ed09c6dbb75","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:13:35.750054Z","iopub.execute_input":"2025-03-11T13:13:35.750339Z","iopub.status.idle":"2025-03-11T13:13:35.765501Z","shell.execute_reply.started":"2025-03-11T13:13:35.750308Z","shell.execute_reply":"2025-03-11T13:13:35.764735Z"}},"outputs":[],"execution_count":10},{"id":"d3d91004-5595-4146-a064-2921d16741bf","cell_type":"code","source":"# for valadtion set\ndef func(model,model_name,xval,yval):\n  pred_val  = model.predict(xval)\n  y_pred_classes_val  = np.round(pred_val).astype(int)\n  pred_clean = np.nan_to_num(pred_val, nan=0.0)  # Replace NaNs with 0 in pred\n  ytest_cnn_clean = np.nan_to_num(yval, nan=0.0)  # Replace NaNs with 0 in ytest_cnn\n  Metrics = []\n  Metrics = pd.DataFrame(Metrics)\n  Metrics['Model'] = 'model'\n  Metrics['Accuracy'] = 'Accuracy'\n  Metrics['mcc'] = 'mcc'\n  Metrics['Kappa'] = 'Kappa'\n  Metrics['precision'] = 'precision'\n  Metrics['recall'] = 'recall'\n  Metrics['f1'] = 'f1'\n  Metrics['sensitivity'] = 'sensitivity'\n  Metrics['specificity'] = 'specificity'\n  Metrics['auc'] = 'auc'\n  Metrics['loss'] = 'loss'\n  Accuracy = accuracy_score(y_val,y_pred_classes_val)\n  mcc = matthews_corrcoef(y_val,y_pred_classes_val)\n  cm1 = confusion_matrix(y_val,y_pred_classes_val)\n  kappa = cohen_kappa_score(y_val,y_pred_classes_val)\n  f1 = f1_score(y_val,y_pred_classes_val, average='macro')\n  precision = precision_score(y_val,y_pred_classes_val, average='macro')\n  recall = recall_score(y_val,y_pred_classes_val, average='macro')\n  sensitivity = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n  specificity = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n  auc = roc_auc_score(y_val,y_pred_classes_val, multi_class='ovr')\n  loss = log_loss(y_val,y_pred_classes_val)\n  Metrics.loc[len(Metrics.index)] = [model_name,Accuracy, mcc, kappa, precision,recall, f1, sensitivity,specificity,auc,loss]\n  return Metrics","metadata":{"id":"5X6LNq86zHkM","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:13:41.386290Z","iopub.execute_input":"2025-03-11T13:13:41.386729Z","iopub.status.idle":"2025-03-11T13:13:41.396611Z","shell.execute_reply.started":"2025-03-11T13:13:41.386689Z","shell.execute_reply":"2025-03-11T13:13:41.395553Z"}},"outputs":[],"execution_count":11},{"id":"035e9741-a50f-4329-93da-7b0234fea369","cell_type":"code","source":"def store_roc_metrics(model, xval, yval, model_name):\n    \"\"\"\n    Store necessary metrics for ROC curve plotting in a single pandas DataFrame.\n    \n    Parameters:\n    ----------\n    model : trained model object\n        The trained machine learning model with a predict_proba method\n    xval : array-like\n        Validation/test features\n    yval : array-like\n        Ground truth labels for validation/test set\n    model_name : str\n        Name of the model for identification\n        \n    Returns:\n    -------\n    DataFrame\n        Single DataFrame containing all necessary metrics to plot ROC curve\n    \"\"\"\n    from sklearn.metrics import roc_curve, auc, roc_auc_score\n    import numpy as np\n    import pandas as pd\n    \n    # Generate probability predictions\n    try:\n        # For most scikit-learn models and keras models with predict_proba\n        ypred_proba = model.predict_proba(xval)\n        \n        # Handle multi-class case - take the positive class probability\n        if ypred_proba.shape[1] > 1:\n            ypred_proba = ypred_proba[:, 1]\n    except (AttributeError, IndexError):\n        try:\n            # For models that return a direct prediction without predict_proba\n            ypred_proba = model.predict(xval)\n            \n            # Ensure predictions are in probability format (0-1 range)\n            if np.max(ypred_proba) > 1 or np.min(ypred_proba) < 0:\n                raise ValueError(\"Model predictions are not probabilities\")\n                \n        except AttributeError:\n            raise AttributeError(\"Model must have either predict_proba or predict method\")\n    \n    # Get hard predictions (for metrics that need them)\n    ypred = (ypred_proba > 0.5).astype(int)\n    \n    # Clean data to handle NaN values\n    yval_clean = np.nan_to_num(yval, nan=0.0)\n    ypred_proba_clean = np.nan_to_num(ypred_proba, nan=0.0)\n    \n    # Calculate ROC curve points\n    fpr, tpr, thresholds = roc_curve(yval_clean, ypred_proba_clean)\n    roc_auc = auc(fpr, tpr)\n    \n    # Create a single DataFrame with all the necessary data\n    # First, create a dataframe with the ROC curve data\n    roc_df = pd.DataFrame({\n        'model_name': [model_name] * len(fpr),\n        'fpr': fpr,\n        'tpr': tpr,\n        'thresholds': np.append(thresholds, np.nan),  # Add NaN to match length of fpr/tpr\n        'auc': [roc_auc] * len(fpr),  # Repeat AUC value for each row\n        'data_type': ['roc_curve'] * len(fpr)  # Tag these rows as ROC curve data\n    })\n    \n    # Then create a dataframe with the prediction data\n    pred_df = pd.DataFrame({\n        'model_name': [model_name] * len(yval_clean),\n        'sample_index': range(len(yval_clean)),\n        'true_label': yval_clean,\n        'predicted_proba': ypred_proba_clean,\n        'predicted_class': ypred,\n        'data_type': ['prediction'] * len(yval_clean)  # Tag these rows as prediction data\n    })\n    \n    # Create a summary row with key statistics\n    summary_df = pd.DataFrame({\n        'model_name': [model_name],\n        'auc': [roc_auc],\n        'data_type': ['summary'],\n        'num_samples': [len(yval_clean)]\n    })\n    \n    # Combine all dataframes into one\n    # We'll handle the different column structures by filling missing values with NaN\n    combined_df = pd.concat([roc_df, pred_df, summary_df], ignore_index=True)\n    \n    return combined_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:49:35.766859Z","iopub.execute_input":"2025-03-11T13:49:35.767165Z","iopub.status.idle":"2025-03-11T13:49:35.775338Z","shell.execute_reply.started":"2025-03-11T13:49:35.767140Z","shell.execute_reply":"2025-03-11T13:49:35.774520Z"}},"outputs":[],"execution_count":22},{"id":"094af6d6-ce0b-47c3-94d0-f0eee71f1e4b","cell_type":"code","source":"def save_metrics(metrics_df, file_path):\n    \"\"\"\n    Check if a file exists and contains data. If not, write the data; otherwise, append it.\n    \n    Args:\n        metrics_df (DataFrame): The DataFrame containing metrics to save.\n        file_path (str): Path to the CSV file.\n    \"\"\"\n    # Check if the file exists and has data\n    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n        # Append to the file\n        metrics_df.to_csv(file_path, mode='a', index=False, header=False)\n        print(f\"Appended data to {file_path}.\")\n    else:\n        # Write to the file\n        metrics_df.to_csv(file_path, index=False)\n        print(f\"Created and wrote data to {file_path}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:13:45.865337Z","iopub.execute_input":"2025-03-11T13:13:45.865708Z","iopub.status.idle":"2025-03-11T13:13:45.870202Z","shell.execute_reply.started":"2025-03-11T13:13:45.865676Z","shell.execute_reply":"2025-03-11T13:13:45.869317Z"}},"outputs":[],"execution_count":13},{"id":"e768dbc7-9a7d-4d40-92ea-f491da3572b1","cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, BatchNormalization, Flatten, Dense, Dropout, Reshape, LSTM\nfrom tensorflow.keras.optimizers import Adam\n\ndef build_model(input_shape, filters=32, kernel_size=3, pool_size=2, lstm_units=32, dense_units=64, dropout_rate=0.3, learning_rate=1e-3):\n    \"\"\"\n    Function to define a CNN-LSTM model with the given hyperparameters.\n\n    Args:\n        input_shape (tuple): Shape of the input data (e.g., (num_features, 1)).\n        filters (int): Number of filters in the initial Conv1D layers.\n        kernel_size (int): Kernel size for the Conv1D layers.\n        pool_size (int): Pool size for MaxPooling1D layers.\n        lstm_units (int): Number of units in the LSTM layers.\n        dense_units (int): Number of units in the Dense layers.\n        dropout_rate (float): Dropout rate for regularization.\n        learning_rate (float): Learning rate for the optimizer.\n\n    Returns:\n        model (Sequential): A compiled Keras Sequential model.\n    \"\"\"\n    model = Sequential([\n        # Input layer\n        Input(shape=input_shape),\n\n        # CNN layers\n        Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'),\n        BatchNormalization(),\n        MaxPooling1D(pool_size=pool_size),\n\n        Conv1D(filters=filters * 2, kernel_size=kernel_size, activation='relu'),\n        BatchNormalization(),\n\n        Conv1D(filters=filters * 4, kernel_size=kernel_size, activation='relu'),\n        BatchNormalization(),\n\n        Conv1D(filters=filters * 2, kernel_size=kernel_size, activation='relu'),\n        BatchNormalization(),\n        MaxPooling1D(pool_size=pool_size),\n\n        # Fully connected layer to reshape for LSTM\n        Flatten(),\n        Dense(dense_units, activation='relu'),\n        Dropout(dropout_rate),\n        Reshape((8, 8)),  # Reshape before LSTM (ensure the output size matches this)\n\n        # LSTM layers\n        LSTM(lstm_units, return_sequences=True),\n        BatchNormalization(),\n        Dropout(dropout_rate),\n\n        LSTM(lstm_units * 2, return_sequences=True),\n        BatchNormalization(),\n        Dropout(dropout_rate),\n\n        LSTM(lstm_units * 4),\n        BatchNormalization(),\n        Dropout(dropout_rate),\n\n        # Fully connected layers\n        Dense(dense_units, activation='relu'),\n        Dropout(dropout_rate),\n        Dense(dense_units // 2, activation='relu'),\n        Dropout(dropout_rate),\n\n        # Output layer for binary classification\n        Dense(1, activation='sigmoid')\n    ])\n\n    # Compile the model\n    optimizer = Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:13:52.490950Z","iopub.execute_input":"2025-03-11T13:13:52.491245Z","iopub.status.idle":"2025-03-11T13:13:52.499704Z","shell.execute_reply.started":"2025-03-11T13:13:52.491221Z","shell.execute_reply":"2025-03-11T13:13:52.498879Z"}},"outputs":[],"execution_count":14},{"id":"f657387b-da58-4769-aa3c-c81cb6c56bc2","cell_type":"markdown","source":"# APAAC","metadata":{}},{"id":"0e83596c-0f23-4153-8ad3-03c9901cd239","cell_type":"code","source":"df = pd.read_csv(r'/kaggle/input/APAAC Train New Whole.csv')\ncolumns = df.columns.tolist()\n# Filter the columns to remove data we do not want\ncolumns = [c for c in columns if c not in [\"target\"]]\n# Store the variable we are predicting\ntarget = \"target\"\nX = df[columns]\nY = df[target]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:14:20.846000Z","iopub.execute_input":"2025-03-11T13:14:20.846316Z","iopub.status.idle":"2025-03-11T13:14:21.020900Z","shell.execute_reply.started":"2025-03-11T13:14:20.846287Z","shell.execute_reply":"2025-03-11T13:14:21.019991Z"}},"outputs":[],"execution_count":16},{"id":"23f8eead-21d1-413a-9c0e-d80406c4241d","cell_type":"code","source":"xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.20,random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:14:23.905687Z","iopub.execute_input":"2025-03-11T13:14:23.906008Z","iopub.status.idle":"2025-03-11T13:14:23.917391Z","shell.execute_reply.started":"2025-03-11T13:14:23.905979Z","shell.execute_reply":"2025-03-11T13:14:23.916391Z"}},"outputs":[],"execution_count":17},{"id":"4d5f72b9-5b9e-4058-a8af-6cb7bd87b15b","cell_type":"code","source":"xtrain_cnn = xtrain.to_numpy()\nytrain_cnn = ytrain.to_numpy()\nxtrain_cnn = xtrain_cnn.reshape(xtrain.shape[0], xtrain.shape[1], 1)\nxtest_cnn = xtest.to_numpy()\nytest_cnn = ytest.to_numpy()\nxtest_cnn = xtest_cnn.reshape(xtest_cnn.shape[0], xtest_cnn.shape[1], 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:14:25.275025Z","iopub.execute_input":"2025-03-11T13:14:25.275347Z","iopub.status.idle":"2025-03-11T13:14:25.279976Z","shell.execute_reply.started":"2025-03-11T13:14:25.275317Z","shell.execute_reply":"2025-03-11T13:14:25.279039Z"}},"outputs":[],"execution_count":18},{"id":"61407292-39c0-4920-a16a-28e108ba8694","cell_type":"markdown","source":"## Cross Validation Training","metadata":{}},{"id":"86e9a808-0b5a-49a8-84c1-024a17c21a09","cell_type":"code","source":"model_name = \"APAAC+model5\"\nfilters = 64\nkernel = 2\npool_size = 2\nlstm_units = filters*2\n\nepochs = 100\nbatch_size = 32\nlearning_rate = 1e-3\n\n# Initialize KFold Cross-Validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# To store metrics for all validation folds\nall_validation_metrics = pd.DataFrame()\nroc_curve_cv = pd.DataFrame()\nroc_curve_ts = pd.DataFrame()\n# Perform KFold Cross-Validation\nfold_no = 1\nfor train_index, val_index in kf.split(xtrain_cnn):\n    # Split data into training and validation sets\n    X_train, X_val = xtrain_cnn[train_index], xtrain_cnn[val_index]\n    y_train, y_val = ytrain_cnn[train_index], ytrain_cnn[val_index]\n\n    # Build a new model for this fold\n    model = build_model(input_shape=(X_train.shape[1], 1))\n\n    # Early stopping to avoid overfitting\n    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9)\n    # Train the model on the training fold\n    print(f\"Training fold {fold_no}...\")\n    model.fit(X_train, y_train,\n              validation_data=(X_val, y_val),\n              epochs=epochs,\n              batch_size=batch_size,\n              callbacks=[early_stopping, reduce_lr],\n              verbose=1)\n\n    # Evaluate on validation fold using func()\n    validation_metrics = func(model, f\"APAAC_Model5_Fold_{fold_no}\", X_val, y_val)\n    all_validation_metrics = pd.concat([all_validation_metrics, validation_metrics], ignore_index=True)\n    roc_cv = store_roc_metrics (model, X_val, y_val, f\"APAAC_Model5_Fold_{fold_no}\")\n    roc_curve_cv = pd.concat([roc_curve_cv, roc_cv], ignore_index=True)\n    fold_no += 1\n\n# Cross-validation results\nprint(\"\\nValidation Metrics for All Folds:\")\nprint(all_validation_metrics)\n\n# Average validation metrics across folds\nmean_validation_metrics = all_validation_metrics.mean(numeric_only=True)\nprint(\"\\nAverage Validation Metrics Across All Folds:\")\nprint(mean_validation_metrics)\n\n\nmean_validation_metrics = roc_curve_cv.mean(numeric_only=True)\nprint(\"\\nAverage ROC Curve Across All Folds:\")\nprint(mean_validation_metrics)","metadata":{"id":"d7339e9b-67f2-423a-9dba-7541a77f1e3d","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:49:39.818088Z","iopub.execute_input":"2025-03-11T13:49:39.818376Z","iopub.status.idle":"2025-03-11T13:51:32.244481Z","shell.execute_reply.started":"2025-03-11T13:49:39.818352Z","shell.execute_reply":"2025-03-11T13:51:32.243308Z"}},"outputs":[{"name":"stdout","text":"Training fold 1...\nEpoch 1/100\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 17ms/step - accuracy: 0.6161 - loss: 0.6809 - val_accuracy: 0.7915 - val_loss: 0.4799 - learning_rate: 0.0010\nEpoch 2/100\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.7824 - loss: 0.4790 - val_accuracy: 0.8035 - val_loss: 0.4162 - learning_rate: 0.0010\nEpoch 3/100\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.8090 - loss: 0.4335 - val_accuracy: 0.8019 - val_loss: 0.4144 - learning_rate: 0.0010\nEpoch 4/100\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.8144 - loss: 0.4190 - val_accuracy: 0.8184 - val_loss: 0.4003 - learning_rate: 0.0010\nEpoch 5/100\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.8260 - loss: 0.3995 - val_accuracy: 0.7944 - val_loss: 0.4259 - learning_rate: 0.0010\nEpoch 6/100\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.8190 - loss: 0.4107 - val_accuracy: 0.8120 - val_loss: 0.4063 - learning_rate: 0.0010\nEpoch 7/100\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.8310 - loss: 0.3912 - val_accuracy: 0.8175 - val_loss: 0.4004 - learning_rate: 0.0010\nEpoch 8/100\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.8373 - loss: 0.3821 - val_accuracy: 0.8318 - val_loss: 0.3883 - learning_rate: 0.0010\nEpoch 9/100\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.8451 - loss: 0.3783 - val_accuracy: 0.8295 - val_loss: 0.3797 - learning_rate: 0.0010\nEpoch 10/100\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.8363 - loss: 0.3757 - val_accuracy: 0.8279 - val_loss: 0.3830 - learning_rate: 0.0010\nEpoch 11/100\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.8415 - loss: 0.3710 - val_accuracy: 0.8379 - val_loss: 0.3733 - learning_rate: 0.0010\nEpoch 12/100\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.8462 - loss: 0.3603 - val_accuracy: 0.8396 - val_loss: 0.3692 - learning_rate: 0.0010\nEpoch 13/100\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.8487 - loss: 0.3596 - val_accuracy: 0.8227 - val_loss: 0.4062 - learning_rate: 0.0010\nEpoch 14/100\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.8439 - loss: 0.3561 - val_accuracy: 0.8230 - val_loss: 0.3778 - learning_rate: 0.0010\nEpoch 15/100\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.8501 - loss: 0.3470 - val_accuracy: 0.8077 - val_loss: 0.4079 - learning_rate: 0.0010\nEpoch 16/100\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.8464 - loss: 0.3495 - val_accuracy: 0.8327 - val_loss: 0.3766 - learning_rate: 0.0010\nEpoch 17/100\n\u001b[1m385/385\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - accuracy: 0.8525 - loss: 0.3480 - val_accuracy: 0.8256 - val_loss: 0.3972 - learning_rate: 0.0010\n\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n\u001b[1m97/97\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-a91c6279a3b5>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mvalidation_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"APAAC_Model5_Fold_{fold_no}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mall_validation_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_validation_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_metrics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mroc_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore_roc_metrics\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"APAAC_Model5_Fold_{fold_no}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mroc_curve_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroc_curve_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_cv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mfold_no\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-462ea77d618d>\u001b[0m in \u001b[0;36mstore_roc_metrics\u001b[0;34m(model, xval, yval, model_name)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Create a single DataFrame with all the necessary data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# First, create a dataframe with the ROC curve data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     roc_df = pd.DataFrame({\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;34m'model_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;34m'fpr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All arrays must be of the same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"],"ename":"ValueError","evalue":"All arrays must be of the same length","output_type":"error"}],"execution_count":23},{"id":"2878a9ca-0c3c-4adb-a33a-b21800423882","cell_type":"code","source":"# Train the final model on the entire training set\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9)\nprint(\"\\nTraining the final model on the full training set...\")\nfinal_model = build_model(input_shape=(xtrain_cnn.shape[1], 1))\n\n\n\n# Train the final model\nhistory = final_model.fit(xtrain_cnn, ytrain_cnn, epochs=epochs, batch_size=batch_size, validation_split=0.1,\n              callbacks=[early_stopping, reduce_lr], verbose=1)\n\n# Evaluate the final model on the test set using total_matric_cal()\nprint(\"\\nEvaluating the final model on the test set...\")\npred = final_model.predict(xtest_cnn)  # Predict probabilities\ny_pred_classes = np.round(pred).astype(int)  # Convert to binary class labels\n\ntest_metrics = total_matric_cal(ytest_cnn, y_pred_classes, pred, \"APAAC_Model5_Test_Set\")\nprint(\"\\nTest Set Metrics:\")\nprint(test_metrics)\n\nroc_cv = store_roc_metrics (final_model, xtest_cnn, ytest_cnn, f\"APAAC_Model5_test\")\nroc_curve_ts = pd.concat([roc_curve_ts, roc_cv], ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:39:17.097017Z","iopub.status.idle":"2025-03-11T13:39:17.097298Z","shell.execute_reply":"2025-03-11T13:39:17.097187Z"}},"outputs":[],"execution_count":null},{"id":"4ce28080-31a0-400d-9660-7f7655880a09","cell_type":"code","source":"# Save the results to CSV files\nsave_metrics(all_validation_metrics, validation_file)\nsave_metrics(test_metrics, file_name)\nsave_metrics(roc_curve_cv, roc_curve_cv_file)\nsave_metrics(roc_curve_ts, roc_curve_ts_file)\n# Accessing training and validation metrics\nprint(history.history.keys())  # Lists all available metrics, e.g., ['loss', 'accuracy', 'val_loss', 'val_accuracy']\n\n# Accessing loss and accuracy values for each epoch\ntraining_loss = history.history['loss']  # Training loss for each epoch\ntraining_accuracy = history.history['accuracy']  # Training accuracy for each epoch\nvalidation_loss = history.history.get('val_loss')  # Validation loss for each epoch (if validation data is provided)\nvalidation_accuracy = history.history.get('val_accuracy')  # Validation accuracy for each epoch (if provided)\n\n# Example: Print the loss and accuracy for the last epoch\nprint(f\"Final Training Loss: {training_loss[-1]}\")\nprint(f\"Final Training Accuracy: {training_accuracy[-1]}\")\nif validation_loss:\n    print(f\"Final Validation Loss: {validation_loss[-1]}\")\nif validation_accuracy:\n    print(f\"Final Validation Accuracy: {validation_accuracy[-1]}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":699},"id":"4408429a-476b-4ffe-9112-b9d7b7fc43d1","outputId":"7810d721-244e-4551-c6b7-dbbfdb2cd7a3","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:39:17.098175Z","iopub.status.idle":"2025-03-11T13:39:17.098578Z","shell.execute_reply":"2025-03-11T13:39:17.098389Z"}},"outputs":[],"execution_count":null},{"id":"11b0f720-689b-4a51-95fa-07fcc8642bc6","cell_type":"markdown","source":"# CTDC","metadata":{}},{"id":"80b9abc7-2607-470a-9922-b8937fc6238a","cell_type":"code","source":"df = pd.read_csv(r'/kaggle/input/CTDC Whole new Train.csv')\ncolumns = df.columns.tolist()\n# Filter the columns to remove data we do not want\ncolumns = [c for c in columns if c not in [\"target\"]]\n# Store the variable we are predicting\ntarget = \"target\"\nX = df[columns]\nY = df[target]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"db71a221-8e1e-47c1-af6b-88c789108bff","cell_type":"code","source":"xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.20,random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"57fa7a27-832f-4782-9467-88ae5d39660f","cell_type":"code","source":"xtrain_cnn = xtrain.to_numpy()\nytrain_cnn = ytrain.to_numpy()\nxtrain_cnn = xtrain_cnn.reshape(xtrain.shape[0], xtrain.shape[1], 1)\nxtest_cnn = xtest.to_numpy()\nytest_cnn = ytest.to_numpy()\nxtest_cnn = xtest_cnn.reshape(xtest_cnn.shape[0], xtest_cnn.shape[1], 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c41a91ef-b693-42fc-94af-9fddfb750504","cell_type":"code","source":"model_no = \"model1\"\nmodel_name = \"CTDC\"\nfilters = 64\nkernel = 2\npool_size = 2\nlstm_units = filters*2\n\nephocs = 100\nbatch_size = 32\nlearning_rate = 1e-3\n\n# Initialize KFold Cross-Validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# To store metrics for all validation folds\nall_validation_metrics = pd.DataFrame()\n\n# Perform KFold Cross-Validation\nfold_no = 1\nfor train_index, val_index in kf.split(xtrain_cnn):\n    # Split data into training and validation sets\n    X_train, X_val = xtrain_cnn[train_index], xtrain_cnn[val_index]\n    y_train, y_val = ytrain_cnn[train_index], ytrain_cnn[val_index]\n\n    # Define the model\n    model = build_model(input_shape=(X_train.shape[1], 1))\n\n    # Early stopping to avoid overfitting\n    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9)\n    # Train the model on the training fold\n    print(f\"Training fold {fold_no}...\")\n    model.fit(X_train, y_train,\n              validation_data=(X_val, y_val),\n              epochs=epochs,\n              batch_size=batch_size,\n              callbacks=[early_stopping, reduce_lr],\n              verbose=1)\n\n    # Evaluate on validation fold using func()\n    validation_metrics = func(model, f\"CTDC_Model5_Fold_{fold_no}\", X_val, y_val)\n    all_validation_metrics = pd.concat([all_validation_metrics, validation_metrics], ignore_index=True)\n    roc_cv = store_roc_metrics (model, X_val, y_val, f\"CTDC_Model5_Fold_{fold_no}\")\n    roc_curve_cv = pd.concat([roc_curve_cv, roc_cv], ignore_index=True)\n    fold_no += 1\n\n# Cross-validation results\nprint(\"\\nValidation Metrics for All Folds:\")\nprint(all_validation_metrics)\n\n# Average validation metrics across folds\nmean_validation_metrics = all_validation_metrics.mean(numeric_only=True)\nprint(\"\\nAverage Validation Metrics Across All Folds:\")\nprint(mean_validation_metrics)\nmean_validation_metrics = roc_curve_cv.mean(numeric_only=True)\nprint(\"\\nAverage ROC Curve Across All Folds:\")\nprint(mean_validation_metrics)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"80f5ea21-5fd2-4b78-ab05-a8b338002337","cell_type":"code","source":"# Train the final model on the entire training set\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9)\nprint(\"\\nTraining the final model on the full training set...\")\nfinal_model = build_model(input_shape=(xtrain_cnn.shape[1], 1))\n\n# Train the final model\nhistory = final_model.fit(xtrain_cnn, ytrain_cnn, epochs=epochs, batch_size=batch_size, validation_split=0.1,\n              callbacks=[early_stopping, reduce_lr], verbose=1)\n\n# Evaluate the final model on the test set using total_matric_cal()\nprint(\"\\nEvaluating the final model on the test set...\")\npred = final_model.predict(xtest_cnn)  # Predict probabilities\ny_pred_classes = np.round(pred).astype(int)  # Convert to binary class labels\n\ntest_metrics = total_matric_cal(ytest_cnn, y_pred_classes, pred, \"CTDC_Model5_Test_Set\")\nprint(\"\\nTest Set Metrics:\")\nprint(test_metrics)\n\nroc_cv = store_roc_metrics (final_model, xtest_cnn, ytest_cnn, f\"CTDC_Model5_test\")\nroc_curve_ts = pd.concat([roc_curve_ts, roc_cv], ignore_index=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3cb96f16-3319-4de3-a1c8-14279d17c2b4","cell_type":"code","source":"# Save the results to CSV files\nsave_metrics(all_validation_metrics, validation_file)\nsave_metrics(test_metrics, file_name)\nsave_metrics(roc_curve_cv, roc_curve_cv_file)\nsave_metrics(roc_curve_ts, roc_curve_ts_file)\n# Accessing training and validation metrics\nprint(history.history.keys())  # Lists all available metrics, e.g., ['loss', 'accuracy', 'val_loss', 'val_accuracy']\n\n# Accessing loss and accuracy values for each epoch\ntraining_loss = history.history['loss']  # Training loss for each epoch\ntraining_accuracy = history.history['accuracy']  # Training accuracy for each epoch\nvalidation_loss = history.history.get('val_loss')  # Validation loss for each epoch (if validation data is provided)\nvalidation_accuracy = history.history.get('val_accuracy')  # Validation accuracy for each epoch (if provided)\n\n# Example: Print the loss and accuracy for the last epoch\nprint(f\"Final Training Loss: {training_loss[-1]}\")\nprint(f\"Final Training Accuracy: {training_accuracy[-1]}\")\nif validation_loss:\n    print(f\"Final Validation Loss: {validation_loss[-1]}\")\nif validation_accuracy:\n    print(f\"Final Validation Accuracy: {validation_accuracy[-1]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"2a28b53d-34a4-48c1-af29-6ed547b10ed8","cell_type":"markdown","source":"# DDE","metadata":{}},{"id":"72acd078-d1ef-4011-94d5-41b4abfa2784","cell_type":"code","source":"df = pd.read_csv(r'/kaggle/input/DDE Whole new Train.csv')\ncolumns = df.columns.tolist()\n# Filter the columns to remove data we do not want\ncolumns = [c for c in columns if c not in [\"target\"]]\n# Store the variable we are predicting\ntarget = \"target\"\nX = df[columns]\nY = df[target]\nxtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.20,random_state=42)\nxtrain_cnn = xtrain.to_numpy()\nytrain_cnn = ytrain.to_numpy()\nxtrain_cnn = xtrain_cnn.reshape(xtrain.shape[0], xtrain.shape[1], 1)\nxtest_cnn = xtest.to_numpy()\nytest_cnn = ytest.to_numpy()\nxtest_cnn = xtest_cnn.reshape(xtest_cnn.shape[0], xtest_cnn.shape[1], 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4c962301-2267-4955-b85d-3846bd802cf7","cell_type":"code","source":"model_no = \"model1\"\nmodel_name = \"DDE\"\nfilters = 64\nkernel = 2\npool_size = 2\nlstm_units = filters*2\n\nephocs = 100\nbatch_size = 32\nlearning_rate = 1e-3\n\n# Initialize KFold Cross-Validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# To store metrics for all validation folds\nall_validation_metrics = pd.DataFrame()\n\n# Perform KFold Cross-Validation\nfold_no = 1\nfor train_index, val_index in kf.split(xtrain_cnn):\n    # Split data into training and validation sets\n    X_train, X_val = xtrain_cnn[train_index], xtrain_cnn[val_index]\n    y_train, y_val = ytrain_cnn[train_index], ytrain_cnn[val_index]\n\n    # Define the model\n    model = build_model(input_shape=(X_train.shape[1], 1))\n\n    # Early stopping to avoid overfitting\n    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9)\n    # Train the model on the training fold\n    print(f\"Training fold {fold_no}...\")\n    model.fit(X_train, y_train,\n              validation_data=(X_val, y_val),\n              epochs=epochs,\n              batch_size=batch_size,\n              callbacks=[early_stopping, reduce_lr],\n              verbose=1)\n\n    # Evaluate on validation fold using func()\n    validation_metrics = func(model, f\"DDE_Model5_Fold_{fold_no}\", X_val, y_val)\n    all_validation_metrics = pd.concat([all_validation_metrics, validation_metrics], ignore_index=True)\n    roc_cv = store_roc_metrics (model, X_val, y_val, f\"DDE_Model5_Fold_{fold_no}\")\n    roc_curve_cv = pd.concat([roc_curve_cv, roc_cv], ignore_index=True)\n    fold_no += 1\n\n# Cross-validation results\nprint(\"\\nValidation Metrics for All Folds:\")\nprint(all_validation_metrics)\n\n# Average validation metrics across folds\nmean_validation_metrics = all_validation_metrics.mean(numeric_only=True)\nprint(\"\\nAverage Validation Metrics Across All Folds:\")\nprint(mean_validation_metrics)\n\nmean_validation_metrics = roc_curve_cv.mean(numeric_only=True)\nprint(\"\\nAverage ROC Curve Across All Folds:\")\nprint(mean_validation_metrics)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"75a9629a-fe49-4a0d-a921-6fad52a5831b","cell_type":"code","source":"# Train the final model on the entire training set\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9)\nprint(\"\\nTraining the final model on the full training set...\")\nfinal_model = build_model(input_shape=(xtrain_cnn.shape[1], 1))\n\n# Train the final model\nhistory = final_model.fit(xtrain_cnn, ytrain_cnn, epochs=epochs, batch_size=batch_size, validation_split=0.1,\n              callbacks=[early_stopping, reduce_lr], verbose=1)\n\n# Evaluate the final model on the test set using total_matric_cal()\nprint(\"\\nEvaluating the final model on the test set...\")\npred = final_model.predict(xtest_cnn)  # Predict probabilities\ny_pred_classes = np.round(pred).astype(int)  # Convert to binary class labels\n\ntest_metrics = total_matric_cal(ytest_cnn, y_pred_classes, pred, \"DDE_Model5_Test_Set\")\nprint(\"\\nTest Set Metrics:\")\nprint(test_metrics)\n\nroc_cv = store_roc_metrics (final_model, xtest_cnn, ytest_cnn, f\"DDE_Model5_test\")\nroc_curve_ts = pd.concat([roc_curve_ts, roc_cv], ignore_index=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b7f5a0b7-4bef-4ea2-9de5-82ada3af1503","cell_type":"code","source":"# Save the results to CSV files\nsave_metrics(all_validation_metrics, validation_file)\nsave_metrics(test_metrics, file_name)\nsave_metrics(roc_curve_cv, roc_curve_cv_file)\nsave_metrics(roc_curve_ts, roc_curve_ts_file)\n# Accessing training and validation metrics\nprint(history.history.keys())  # Lists all available metrics, e.g., ['loss', 'accuracy', 'val_loss', 'val_accuracy']\n\n# Accessing loss and accuracy values for each epoch\ntraining_loss = history.history['loss']  # Training loss for each epoch\ntraining_accuracy = history.history['accuracy']  # Training accuracy for each epoch\nvalidation_loss = history.history.get('val_loss')  # Validation loss for each epoch (if validation data is provided)\nvalidation_accuracy = history.history.get('val_accuracy')  # Validation accuracy for each epoch (if provided)\n\n# Example: Print the loss and accuracy for the last epoch\nprint(f\"Final Training Loss: {training_loss[-1]}\")\nprint(f\"Final Training Accuracy: {training_accuracy[-1]}\")\nif validation_loss:\n    print(f\"Final Validation Loss: {validation_loss[-1]}\")\nif validation_accuracy:\n    print(f\"Final Validation Accuracy: {validation_accuracy[-1]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e25818cb-c7af-46a7-a99b-9a2cc1d38b41","cell_type":"markdown","source":"# Distance Pair","metadata":{}},{"id":"e5ceea49-a768-4fa1-9dae-48ea482b8e45","cell_type":"code","source":"df = pd.read_csv(r'/kaggle/input/DistancePair Whole new Train.csv')\ncolumns = df.columns.tolist()\n# Filter the columns to remove data we do not want\ncolumns = [c for c in columns if c not in [\"target\"]]\n# Store the variable we are predicting\ntarget = \"target\"\nX = df[columns]\nY = df[target]\nxtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.20,random_state=42)\nxtrain_cnn = xtrain.to_numpy()\nytrain_cnn = ytrain.to_numpy()\nxtrain_cnn = xtrain_cnn.reshape(xtrain.shape[0], xtrain.shape[1], 1)\nxtest_cnn = xtest.to_numpy()\nytest_cnn = ytest.to_numpy()\nxtest_cnn = xtest_cnn.reshape(xtest_cnn.shape[0], xtest_cnn.shape[1], 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"741e8660-b31f-4fd2-8401-3556a4ed2dc8","cell_type":"code","source":"model_no = \"model1\"\nmodel_name = \"DDE\"\nfilters = 64\nkernel = 2\npool_size = 2\nlstm_units = filters*2\n\nephocs = 100\nbatch_size = 32\nlearning_rate = 1e-3\n\n# Initialize KFold Cross-Validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# To store metrics for all validation folds\nall_validation_metrics = pd.DataFrame()\n\n# Perform KFold Cross-Validation\nfold_no = 1\nfor train_index, val_index in kf.split(xtrain_cnn):\n    # Split data into training and validation sets\n    X_train, X_val = xtrain_cnn[train_index], xtrain_cnn[val_index]\n    y_train, y_val = ytrain_cnn[train_index], ytrain_cnn[val_index]\n\n    model = build_model(input_shape=(X_train.shape[1], 1))\n\n    # Early stopping to avoid overfitting\n    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9)\n    # Train the model on the training fold\n    print(f\"Training fold {fold_no}...\")\n    model.fit(X_train, y_train,\n              validation_data=(X_val, y_val),\n              epochs=epochs,\n              batch_size=batch_size,\n              callbacks=[early_stopping, reduce_lr],\n              verbose=1)\n\n    # Evaluate on validation fold using func()\n    validation_metrics = func(model, f\"DistancePair_Model5_Fold_{fold_no}\", X_val, y_val)\n    all_validation_metrics = pd.concat([all_validation_metrics, validation_metrics], ignore_index=True)\n    roc_cv = store_roc_metrics (model, X_val, y_val, f\"DistancePair_Model5_Fold_{fold_no}\")\n    roc_curve_cv = pd.concat([roc_curve_cv, roc_cv], ignore_index=True)\n    fold_no += 1\n\n# Cross-validation results\nprint(\"\\nValidation Metrics for All Folds:\")\nprint(all_validation_metrics)\n\n# Average validation metrics across folds\nmean_validation_metrics = all_validation_metrics.mean(numeric_only=True)\nprint(\"\\nAverage Validation Metrics Across All Folds:\")\nprint(mean_validation_metrics)\nmean_validation_metrics = roc_curve_cv.mean(numeric_only=True)\nprint(\"\\nAverage ROC Curve Across All Folds:\")\nprint(mean_validation_metrics)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a0f790ea-1df2-4119-a01d-e40553c23da9","cell_type":"code","source":"# Train the final model on the entire training set\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9)\nprint(\"\\nTraining the final model on the full training set...\")\nfinal_model = build_model(input_shape=(xtrain_cnn.shape[1], 1))\n\n# Train the final model\nhistory = final_model.fit(xtrain_cnn, ytrain_cnn, epochs=epochs, batch_size=batch_size, validation_split=0.1,\n              callbacks=[early_stopping, reduce_lr], verbose=1)\n\n# Evaluate the final model on the test set using total_matric_cal()\nprint(\"\\nEvaluating the final model on the test set...\")\npred = final_model.predict(xtest_cnn)  # Predict probabilities\ny_pred_classes = np.round(pred).astype(int)  # Convert to binary class labels\n\ntest_metrics = total_matric_cal(ytest_cnn, y_pred_classes, pred, \"DistancePair_Model5_Test_Set\")\nprint(\"\\nTest Set Metrics:\")\nprint(test_metrics)\nroc_cv = store_roc_metrics (final_model, xtest_cnn, ytest_cnn, f\"DistancePair_Model5_test\")\nroc_curve_ts = pd.concat([roc_curve_ts, roc_cv], ignore_index=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3cb36049-02de-4db8-bd1b-f02633dd1d5a","cell_type":"code","source":"# Save the results to CSV files\nsave_metrics(all_validation_metrics, validation_file)\nsave_metrics(test_metrics, file_name)\nsave_metrics(roc_curve_cv, roc_curve_cv_file)\nsave_metrics(roc_curve_ts, roc_curve_ts_file)\n# Accessing training and validation metrics\nprint(history.history.keys())  # Lists all available metrics, e.g., ['loss', 'accuracy', 'val_loss', 'val_accuracy']\n\n# Accessing loss and accuracy values for each epoch\ntraining_loss = history.history['loss']  # Training loss for each epoch\ntraining_accuracy = history.history['accuracy']  # Training accuracy for each epoch\nvalidation_loss = history.history.get('val_loss')  # Validation loss for each epoch (if validation data is provided)\nvalidation_accuracy = history.history.get('val_accuracy')  # Validation accuracy for each epoch (if provided)\n\n# Example: Print the loss and accuracy for the last epoch\nprint(f\"Final Training Loss: {training_loss[-1]}\")\nprint(f\"Final Training Accuracy: {training_accuracy[-1]}\")\nif validation_loss:\n    print(f\"Final Validation Loss: {validation_loss[-1]}\")\nif validation_accuracy:\n    print(f\"Final Validation Accuracy: {validation_accuracy[-1]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4ba53448-03de-46ec-8df3-f9539c78cfcf","cell_type":"markdown","source":"# PAAC","metadata":{}},{"id":"1e47dc3b-b03f-4fc9-ac98-03f10827c414","cell_type":"code","source":"df = pd.read_csv(r'/kaggle/input/PAAC new Train.csv')\ncolumns = df.columns.tolist()\n# Filter the columns to remove data we do not want\ncolumns = [c for c in columns if c not in [\"target\"]]\n# Store the variable we are predicting\ntarget = \"target\"\nX = df[columns]\nY = df[target]\nxtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.20,random_state=42)\nxtrain_cnn = xtrain.to_numpy()\nytrain_cnn = ytrain.to_numpy()\nxtrain_cnn = xtrain_cnn.reshape(xtrain.shape[0], xtrain.shape[1], 1)\nxtest_cnn = xtest.to_numpy()\nytest_cnn = ytest.to_numpy()\nxtest_cnn = xtest_cnn.reshape(xtest_cnn.shape[0], xtest_cnn.shape[1], 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3c0b835f-b368-4097-8fde-4862704e4148","cell_type":"code","source":"model_no = \"model1\"\nmodel_name = \"PAAC\"\nfilters = 64\nkernel = 2\npool_size = 2\nlstm_units = filters*2\n\nephocs = 100\nbatch_size = 32\nlearning_rate = 1e-3\n\n# Initialize KFold Cross-Validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# To store metrics for all validation folds\nall_validation_metrics = pd.DataFrame()\n\n# Perform KFold Cross-Validation\nfold_no = 1\nfor train_index, val_index in kf.split(xtrain_cnn):\n    # Split data into training and validation sets\n    X_train, X_val = xtrain_cnn[train_index], xtrain_cnn[val_index]\n    y_train, y_val = ytrain_cnn[train_index], ytrain_cnn[val_index]\n\n    model = build_model(input_shape=(X_train.shape[1], 1))\n\n    # Early stopping to avoid overfitting\n    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9)\n    # Train the model on the training fold\n    print(f\"Training fold {fold_no}...\")\n    model.fit(X_train, y_train,\n              validation_data=(X_val, y_val),\n              epochs=epochs,\n              batch_size=batch_size,\n              callbacks=[early_stopping, reduce_lr],\n              verbose=1)\n\n    # Evaluate on validation fold using func()\n    validation_metrics = func(model, f\"PAAC_Model5_Fold_{fold_no}\", X_val, y_val)\n    all_validation_metrics = pd.concat([all_validation_metrics, validation_metrics], ignore_index=True)\n    roc_cv = store_roc_metrics (model, X_val, y_val, f\"CTDC_Model5_Fold_{fold_no}\")\n    roc_curve_cv = pd.concat([roc_curve_cv, roc_cv], ignore_index=True)\n    fold_no += 1\n\n# Cross-validation results\nprint(\"\\nValidation Metrics for All Folds:\")\nprint(all_validation_metrics)\n\n# Average validation metrics across folds\nmean_validation_metrics = all_validation_metrics.mean(numeric_only=True)\nprint(\"\\nAverage Validation Metrics Across All Folds:\")\nprint(mean_validation_metrics)\nmean_validation_metrics = roc_curve_cv.mean(numeric_only=True)\nprint(\"\\nAverage ROC Curve Across All Folds:\")\nprint(mean_validation_metrics)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d2308c7d-afab-434a-965b-44ce3ff1aa12","cell_type":"code","source":"# Train the final model on the entire training set\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9)\nprint(\"\\nTraining the final model on the full training set...\")\nfinal_model = build_model(input_shape=(xtrain_cnn.shape[1], 1))\n\n# Train the final model\nhistory = final_model.fit(xtrain_cnn, ytrain_cnn, epochs=epochs, batch_size=batch_size, validation_split=0.1,\n              callbacks=[early_stopping, reduce_lr], verbose=1)\n\n# Evaluate the final model on the test set using total_matric_cal()\nprint(\"\\nEvaluating the final model on the test set...\")\npred = final_model.predict(xtest_cnn)  # Predict probabilities\ny_pred_classes = np.round(pred).astype(int)  # Convert to binary class labels\n\ntest_metrics = total_matric_cal(ytest_cnn, y_pred_classes, pred, \"PAAC_Model5_Test_Set\")\nprint(\"\\nTest Set Metrics:\")\nprint(test_metrics)\nroc_cv = store_roc_metrics (final_model, xtest_cnn, ytest_cnn, f\"CTDC_Model5_test\")\nroc_curve_ts = pd.concat([roc_curve_ts, roc_cv], ignore_index=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3774597c-3d2f-4592-92dd-fd4d58e31892","cell_type":"code","source":"# Save the results to CSV files\nsave_metrics(all_validation_metrics, validation_file)\nsave_metrics(test_metrics, file_name)\nsave_metrics(roc_curve_cv, roc_curve_cv_file)\nsave_metrics(roc_curve_ts, roc_curve_ts_file)\n# Accessing training and validation metrics\nprint(history.history.keys())  # Lists all available metrics, e.g., ['loss', 'accuracy', 'val_loss', 'val_accuracy']\n\n# Accessing loss and accuracy values for each epoch\ntraining_loss = history.history['loss']  # Training loss for each epoch\ntraining_accuracy = history.history['accuracy']  # Training accuracy for each epoch\nvalidation_loss = history.history.get('val_loss')  # Validation loss for each epoch (if validation data is provided)\nvalidation_accuracy = history.history.get('val_accuracy')  # Validation accuracy for each epoch (if provided)\n\n# Example: Print the loss and accuracy for the last epoch\nprint(f\"Final Training Loss: {training_loss[-1]}\")\nprint(f\"Final Training Accuracy: {training_accuracy[-1]}\")\nif validation_loss:\n    print(f\"Final Validation Loss: {validation_loss[-1]}\")\nif validation_accuracy:\n    print(f\"Final Validation Accuracy: {validation_accuracy[-1]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5d06f5f0-cf02-40ec-86dd-04c637816048","cell_type":"markdown","source":"# New data","metadata":{}},{"id":"a280be1d-84b2-4f62-856b-fc14dee6cf25","cell_type":"code","source":"df = pd.read_csv('/input/protein-dataset/New Data1.csv')\ncolumns = df.columns.tolist()\n# Filter the columns to remove data we do not want\ncolumns = [c for c in columns if c not in [\"target\"]]\n# Store the variable we are predicting\ntarget = \"target\"\nX = df[columns]\nY = df[target]\nxtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.20,random_state=42)\nxtrain_cnn = xtrain.to_numpy()\nytrain_cnn = ytrain.to_numpy()\nxtrain_cnn = xtrain_cnn.reshape(xtrain.shape[0], xtrain.shape[1], 1)\nxtest_cnn = xtest.to_numpy()\nytest_cnn = ytest.to_numpy()\nxtest_cnn = xtest_cnn.reshape(xtest_cnn.shape[0], xtest_cnn.shape[1], 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T13:20:28.175185Z","iopub.execute_input":"2025-01-23T13:20:28.175480Z","iopub.status.idle":"2025-01-23T13:20:28.198867Z","shell.execute_reply.started":"2025-01-23T13:20:28.175460Z","shell.execute_reply":"2025-01-23T13:20:28.197725Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-481156d58de0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/input/protein-dataset/New Data1.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Filter the columns to remove data we do not want\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Store the variable we are predicting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/input/protein-dataset/New Data1.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/input/protein-dataset/New Data1.csv'","output_type":"error"}],"execution_count":18},{"id":"610b7988-afb6-42d5-83be-6fb6b02df191","cell_type":"code","source":"model_no = \"model1\"\nmodel_name = \"New-data\"\nfilters = 64\nkernel = 2\npool_size = 2\nlstm_units = filters*2\n\nephocs = 100\nbatch_size = 32\nlearning_rate = 1e-3\n\n# Initialize KFold Cross-Validation\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# To store metrics for all validation folds\nall_validation_metrics = pd.DataFrame()\n\n# Perform KFold Cross-Validation\nfold_no = 1\nfor train_index, val_index in kf.split(xtrain_cnn):\n    # Split data into training and validation sets\n    X_train, X_val = xtrain_cnn[train_index], xtrain_cnn[val_index]\n    y_train, y_val = ytrain_cnn[train_index], ytrain_cnn[val_index]\n\n    # Define the model\n    model = build_model(input_shape=(X_train.shape[1], 1))\n\n    # Early stopping to avoid overfitting\n    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9)\n    # Train the model on the training fold\n    print(f\"Training fold {fold_no}...\")\n    model.fit(X_train, y_train,\n              validation_data=(X_val, y_val),\n              epochs=epochs,\n              batch_size=batch_size,\n              callbacks=[early_stopping, reduce_lr],\n              verbose=1)\n\n    # Evaluate on validation fold using func()\n    validation_metrics = func(model, f\"New-data_Model5_Fold_{fold_no}\", X_val, y_val)\n    all_validation_metrics = pd.concat([all_validation_metrics, validation_metrics], ignore_index=True)\n    roc_cv = store_roc_metrics (model, X_val, y_val, f\"Merged_Data_Model5_Fold_{fold_no}\")\n    roc_curve_cv = pd.concat([roc_curve_cv, roc_cv], ignore_index=True)\n    fold_no += 1\n\n# Cross-validation results\nprint(\"\\nValidation Metrics for All Folds:\")\nprint(all_validation_metrics)\n\n# Average validation metrics across folds\nmean_validation_metrics = all_validation_metrics.mean(numeric_only=True)\nprint(\"\\nAverage Validation Metrics Across All Folds:\")\nprint(mean_validation_metrics)\n\nmean_validation_metrics = roc_curve_cv.mean(numeric_only=True)\nprint(\"\\nAverage ROC Curve Across All Folds:\")\nprint(mean_validation_metrics)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"cd6b90d5-6e49-4cbe-9411-625b0d8a066b","cell_type":"code","source":"# Train the final model on the entire training set\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-9)\nprint(\"\\nTraining the final model on the full training set...\")\nfinal_model = build_model(input_shape=(xtrain_cnn.shape[1], 1))\n\n# Train the final model\nhistory = final_model.fit(xtrain_cnn, ytrain_cnn, epochs=epochs, batch_size=batch_size, validation_split=0.1,\n              callbacks=[early_stopping, reduce_lr], verbose=1)\n\n# Evaluate the final model on the test set using total_matric_cal()\nprint(\"\\nEvaluating the final model on the test set...\")\npred = final_model.predict(xtest_cnn)  # Predict probabilities\ny_pred_classes = np.round(pred).astype(int)  # Convert to binary class labels\n\ntest_metrics = total_matric_cal(ytest_cnn, y_pred_classes, pred, \"New-data_Model5_Test_Set\")\nprint(\"\\nTest Set Metrics:\")\nprint(test_metrics)\n\nroc_cv = store_roc_metrics (final_model, xtest_cnn, ytest_cnn, f\"Merged_Data_Model5_test\")\nroc_curve_ts = pd.concat([roc_curve_ts, roc_cv], ignore_index=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"45d30db7-799f-4ad0-89af-469371a76e98","cell_type":"code","source":"# Save the results to CSV files\nsave_metrics(all_validation_metrics, validation_file)\nsave_metrics(test_metrics, file_name)\nsave_metrics(roc_curve_cv, roc_curve_cv_file)\nsave_metrics(roc_curve_ts, roc_curve_ts_file)\n# Accessing training and validation metrics\nprint(history.history.keys())  # Lists all available metrics, e.g., ['loss', 'accuracy', 'val_loss', 'val_accuracy']\n\n# Accessing loss and accuracy values for each epoch\ntraining_loss = history.history['loss']  # Training loss for each epoch\ntraining_accuracy = history.history['accuracy']  # Training accuracy for each epoch\nvalidation_loss = history.history.get('val_loss')  # Validation loss for each epoch (if validation data is provided)\nvalidation_accuracy = history.history.get('val_accuracy')  # Validation accuracy for each epoch (if provided)\n\n# Example: Print the loss and accuracy for the last epoch\nprint(f\"Final Training Loss: {training_loss[-1]}\")\nprint(f\"Final Training Accuracy: {training_accuracy[-1]}\")\nif validation_loss:\n    print(f\"Final Validation Loss: {validation_loss[-1]}\")\nif validation_accuracy:\n    print(f\"Final Validation Accuracy: {validation_accuracy[-1]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}